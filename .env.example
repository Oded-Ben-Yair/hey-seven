# ============================================================
# Hey Seven Property Q&A — Environment Configuration
# ============================================================
# Copy this file to .env and fill in real values.
# All variables have sensible defaults except GOOGLE_API_KEY.
# ============================================================

# --- Google AI (REQUIRED) --------------------------------
# Gemini API key from https://aistudio.google.com/apikey
GOOGLE_API_KEY=

# --- Property Configuration ------------------------------
PROPERTY_NAME=Mohegan Sun
PROPERTY_DATA_PATH=data/mohegan_sun.json
PROPERTY_WEBSITE=mohegansun.com
PROPERTY_PHONE=1-888-226-7711             # Property contact phone for fallback messages

# --- LLM Tuning -----------------------------------------
MODEL_NAME=gemini-2.5-flash           # gemini-2.5-pro for complex reasoning
MODEL_TEMPERATURE=0.3                 # 0.0-1.0; lower = more deterministic
MODEL_TIMEOUT=30                      # Seconds before LLM call times out
MODEL_MAX_RETRIES=2                   # Retry count on transient LLM errors
MODEL_MAX_OUTPUT_TOKENS=2048          # Max tokens per response

# --- Embeddings -----------------------------------------
EMBEDDING_MODEL=gemini-embedding-001

# --- RAG Pipeline ----------------------------------------
CHROMA_PERSIST_DIR=data/chroma        # Local ChromaDB storage path
RAG_TOP_K=5                           # Number of chunks to retrieve
RAG_CHUNK_SIZE=800                    # Characters per chunk
RAG_CHUNK_OVERLAP=120                 # Overlap between adjacent chunks (~15% of chunk size)
RAG_MIN_RELEVANCE_SCORE=0.3           # Minimum relevance score (0-1, higher = more relevant)

# --- API Server ------------------------------------------
API_KEY=                                   # When set, /chat requires X-API-Key header
ALLOWED_ORIGINS=["http://localhost:8080"]   # CORS origins (JSON list)
RATE_LIMIT_CHAT=20                    # Max /chat requests per minute per IP
RATE_LIMIT_MAX_CLIENTS=10000          # Max tracked client IPs (memory guard)
SSE_TIMEOUT_SECONDS=60                # SSE stream timeout
MAX_REQUEST_BODY_SIZE=65536           # Max request body in bytes (64 KB)

# --- Agent ------------------------------------------------
MAX_MESSAGE_LIMIT=40                  # Max total messages (human + AI) before forcing conversation end
MAX_HISTORY_MESSAGES=20               # Sliding window: only last N messages sent to LLM
ENABLE_HITL_INTERRUPT=false           # When true, pauses before generate node for human-in-the-loop review
GRAPH_RECURSION_LIMIT=10              # LangGraph recursion limit (validate→retry loop bound)
CB_FAILURE_THRESHOLD=5                # Circuit breaker: consecutive failures to open
CB_COOLDOWN_SECONDS=60                # Circuit breaker: seconds before half-open probe

# --- Observability ---------------------------------------
LOG_LEVEL=INFO                        # DEBUG | INFO | WARNING | ERROR
ENVIRONMENT=development               # development | staging | production
VERSION=0.1.0

# --- LangSmith (Optional) -------------------------------
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=
# LANGCHAIN_PROJECT=hey-seven
